# midagma/notreks.pyfrom __future__ import annotationsfrom typing import Optional, Sequence, Tuple, Union, Literal, Dict, Anyimport numpy as npimport torchfrom dataclasses import dataclass, fieldPairs = Union[np.ndarray, Sequence[Tuple[int, int]]]TrekMode = Literal["off", "log", "opt"]TrekRegularizerNames = ["pst", "tcc", "none"]CyclePenalty = Literal["spectral", "logdet"]PSTPenalty = Literal["exp", "log", "inv", "binom", "logdet"]PerronMethod = Literal["power", "eig_torch", "eig_numpy"]TCCVersion = Literal["DAG_learning", "exact_trek_graph", "exact_original_graph", "approx_trek_graph"]IdxMode = Literal["all", "single_random"]Agg = Literal["mean", "sum", "max", "lse", "none"]# module-level persistent generator_TCC_RNG = torch.Generator()_TCC_RNG.manual_seed(123456)   # fixed seed for reproducibility@dataclass(frozen=True)class TrekRegularizer:    """    Generic trek regularizer interface.    - mode="opt": contributes to objective and gradient    - mode="log": only value computed for logging (no gradient)    - mode="off": disabled    """    name: str    mode: TrekMode = "off"    weight: float = 0.0    cfg: Dict[str, Any] = field(default_factory=dict)    def enabled(self) -> bool:        return self.mode != "off" and self.weight != 0.0@dataclass(frozen=True)class PSTRegularizer(TrekRegularizer):    """    PST trek regularizer wrapper.    cfg should include:      I: pst_I      seq: pst_seq      kwargs: pst_kwargs    """    def __init__(        self,        *,        I,        seq: PSTPenalty = "exp",        weight: float = 0.0,        kwargs: Optional[Dict[str, Any]] = None,        mode: TrekMode = "opt",        name: str = "pst",    ):        object.__setattr__(self, "name", name)        object.__setattr__(self, "mode", mode)        object.__setattr__(self, "weight", float(weight))        object.__setattr__(self, "cfg", {            "I": I,             "seq": seq,             "kwargs": {} if kwargs is None else dict(kwargs)            })@dataclass(frozen=True)class TCCRegularizer(TrekRegularizer):    """    Trek-Cycle Coupling - trek regularizer.    Penalty: rho( [[W2, w*S],[I, W2^T]] ) - rho(W2)    cfg includes:      I: pairs      w: scalar multiplier on S (default 1.0)      n_iter: power iteration steps for rho approximation      eps: small stabilizer for normalization    """    def __init__(        self,        *,        I,        cycle_penalty: CyclePenalty = "spectral", # 'spectral', 'logdet'        version: TCCVersion = "approx_trek_graph",        method: PerronMethod = "eig_torch",        idx_mode: IdxMode = "single_random",        weight: float = 1.0,        w: float = 1.0,        s: float = 1.0,        n_iter: int = 10,        eps: float = 1e-12,        mode: TrekMode = "opt",        name: str = "tcc",    ):        object.__setattr__(self, "cycle_penalty", cycle_penalty)        object.__setattr__(self, "name", name)        object.__setattr__(self, "mode", mode)        object.__setattr__(self, "weight", float(weight))        object.__setattr__(self, "cfg", {            "I": I,            "version": version,            "method": method,            "idx_mode": idx_mode,            "w": float(w),            "n_iter": int(n_iter),            "eps": float(eps),            "s": float(s)        })# -*- coding: utf-8 -*-        def _pairs_to_hashable(I: Any) -> tuple[tuple[int, int], ...]:    I_np = np.asarray(I, dtype=np.int64)    if I_np.size == 0:        return tuple()    if I_np.ndim != 2 or I_np.shape[1] != 2:        raise ValueError("I must be array-like of shape (m,2)")    return tuple(map(tuple, I_np.tolist()))def make_trek_reg(I: np.ndarray, trek_cfg: Dict[str, Any]) -> Any:    """    trek_cfg is the YAML dict under `trek_reg: ...`.    Returns: None | PSTRegularizer | TCCRegularizer    """    if trek_cfg is None:        return None    name = str(trek_cfg.get("name", "none")).strip().lower()        I_hash = _pairs_to_hashable(I)    if name == "none":        return None    elif name == "pst":        # expected keys (with defaults)        weight = float(trek_cfg.get("weight", 0.1))        seq = str(trek_cfg.get("seq", "log"))        mode = str(trek_cfg.get("mode", "opt"))        # PST kwargs (only some sequences need some of them; safe to pass)        K_log = int(trek_cfg.get("K_log", 40))        eps_inv = float(trek_cfg.get("eps_inv", 1e-8))        s = float(trek_cfg.get("s", 5.0))        return PSTRegularizer(            I=I_hash, # I,            seq=seq,            weight=weight,            kwargs={"K_log": K_log, "eps_inv": eps_inv, "s": s},            mode=mode,        )    elif name == "tcc":        weight = float(trek_cfg.get("weight", 0.1))        mode = str(trek_cfg.get("mode", "opt"))        cycle_penalty = str(trek_cfg.get("cycle_penalty", "spectral"))        w = float(trek_cfg.get("w", 100.0))        n_iter = int(trek_cfg.get("n_iter", 10))        eps = float(trek_cfg.get("eps", 1e-12))        # optional knobs (keep if your TCCRegularizer supports them)        version = str(trek_cfg.get("version", "C"))        method = str(trek_cfg.get("method", "eig_torch"))        idx_mode = str(trek_cfg.get("idx_mode", "single_random"))        s_logdet = float(trek_cfg.get("s_logdet", 2.0))        return TCCRegularizer(            I=I_hash, # I,            cycle_penalty=cycle_penalty,            weight=weight,            w=w,            n_iter=n_iter,            eps=eps,            mode=mode,            version=version,            method=method,            idx_mode=idx_mode,            s=s_logdet,   # only used if cycle_penalty="logdet"        )    else:            raise ValueError(f"Unknown trek_reg.name={name!r} (expected none|pst|tcc)")def _pairs_to_torch(I: Pairs, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:    """    Convert I (m,2) into (rows, cols) torch.LongTensors on the given device.    Accepts:      - numpy array of shape (m,2)      - list/tuple of pairs    """    I_np = np.asarray(I, dtype=np.int64)    if I_np.ndim != 2 or I_np.shape[1] != 2:        raise ValueError("I must be an array-like of shape (m, 2) with integer indices.")    rows = torch.as_tensor(I_np[:, 0], device=device, dtype=torch.long)    cols = torch.as_tensor(I_np[:, 1], device=device, dtype=torch.long)    return rows, colsdef _indicator_from_pairs(    I: Pairs,    d: int,    device: torch.device,    dtype: torch.dtype,) -> torch.Tensor:    """    Build S = sum_{(i,j) in I} E_{ij} as a (d,d) tensor (0/1).    """    """    Build S = sum_{(i,j) in I} E_{ij} as a (d,d) tensor (0/1).    """    device = torch.device(device) if not isinstance(device, torch.device) else device    if not isinstance(dtype, torch.dtype):        # convert numpy dtype -> torch dtype        dtype = torch.from_numpy(np.zeros((), dtype=dtype)).dtype    S = torch.zeros((d, d), device=device, dtype=dtype)    I_np = np.asarray(I, dtype=np.int64)    if I_np.size == 0:        return S    if I_np.ndim != 2 or I_np.shape[1] != 2:        raise ValueError("I must be array-like of shape (m,2)")    rows = torch.as_tensor(I_np[:, 0], device=device, dtype=torch.long)    cols = torch.as_tensor(I_np[:, 1], device=device, dtype=torch.long)    S[rows, cols] = 1.0    return Sdef _make_positive_vector(x: torch.Tensor, eps: float = 0.0) -> torch.Tensor:    """    Enforce a consistent sign for an eigenvector: make it 'mostly positive'.    For Perron vectors of nonnegative matrices, there exists a nonnegative representative.    """    # If complex, take real part (we only expect real Perron in your nonnegative setting)    if torch.is_complex(x):        x = x.real    # Choose sign so that sum is positive    if x.sum() < 0:        x = -x    # Optionally clip tiny negatives caused by numerical noise (not required, but can help)    if eps > 0:        x = torch.clamp(x, min=-eps)    return xdef perron_eig_with_gradA(    A: torch.Tensor,    *,    method: PerronMethod = "eig_torch",    n_iter: int = 50,    eps: float = 1e-12,) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:    """    Returns (rho, u, v, G_A) where:      - rho: scalar (detached)      - v: right eigenvector (detached, real)      - u: left eigenvector  (detached, real)  i.e. right eigenvector of A^T      - G_A: gradient d rho / d A = u v^T / (u^T v)   (detached)    No autograd through the eigen computations.    """    if A.ndim != 2 or A.shape[0] != A.shape[1]:        raise ValueError("A must be square")    d = A.shape[0]    device, dtype = A.device, A.dtype    with torch.no_grad():        if method == "power":            # Right v            v = torch.ones((d,), device=device, dtype=dtype)            for _ in range(int(n_iter)):                Av = A @ v                v = Av / (torch.linalg.norm(Av) + eps)            # Left u = Perron of A^T            AT = A.transpose(0, 1)            u = torch.ones((d,), device=device, dtype=dtype)            for _ in range(int(n_iter)):                ATu = AT @ u                u = ATu / (torch.linalg.norm(ATu) + eps)            rho = (v * (A @ v)).sum() / ((v * v).sum() + eps)        elif method == "eig_torch":            # Full eigendecomposition (can be complex)            eigvals, eigvecs = torch.linalg.eig(A)  # (d,), (d,d) columns are eigenvectors            # pick index of max real part            idx = torch.argmax(eigvals.real)            lam = eigvals[idx]            v = eigvecs[:, idx]            # left eigenvector from A^T            eigvals_T, eigvecs_T = torch.linalg.eig(A.transpose(0, 1))            idxT = torch.argmax(eigvals_T.real)            u = eigvecs_T[:, idxT]            rho = lam.real            v = _make_positive_vector(v, eps=0.0)            u = _make_positive_vector(u, eps=0.0)            # cast to real dtype if needed            v = v.to(dtype=dtype)            u = u.to(dtype=dtype)        elif method == "eig_numpy":            # CPU numpy eig (real or complex); bring back to torch            A_np = A.detach().cpu().numpy()            vals, vecs = np.linalg.eig(A_np)            idx = np.argmax(vals.real)            rho = float(vals[idx].real)            v_np = vecs[:, idx]            valsT, vecsT = np.linalg.eig(A_np.T)            idxT = np.argmax(valsT.real)            u_np = vecsT[:, idxT]            v = torch.as_tensor(v_np, device=device, dtype=dtype)            u = torch.as_tensor(u_np, device=device, dtype=dtype)            v = _make_positive_vector(v, eps=0.0)            u = _make_positive_vector(u, eps=0.0)            rho = torch.as_tensor(rho, device=device, dtype=dtype)        else:            raise ValueError("method must be one of {'power','eig_torch','eig_numpy'}")        denom = (u * v).sum() + eps        G_A = torch.outer(u, v) / denom    return rho.detach(), u.detach(), v.detach(), G_A.detach()def logdet_acyc_value_gradA(    A: torch.Tensor,    *,    s: float = 1.0,    eps: float = 1e-12,) -> tuple[torch.Tensor, torch.Tensor]:    """    DAGMA-like logdet acyclicity, adapted to the case where the optimization variable    is already nonnegative (no extra Hadamard square inside the constraint).    h(A) = - logdet(sI - A) + n log(s),  with n = A.shape[0].    grad wrt A: (sI - A)^{-T}.    Returns:      h (scalar tensor)      G_A = dh/dA (same shape as A)    """    if A.ndim != 2 or A.shape[0] != A.shape[1]:        raise ValueError("A must be square")    n = A.shape[0]    device, dtype = A.device, A.dtype    I_n = torch.eye(n, device=device, dtype=dtype)    M = (float(s) * I_n) - A    sign, logabsdet = torch.linalg.slogdet(M)    # If sign <= 0, h is not well-defined as real (outside M-matrix regime).    # You can still return something, but typically you want s large enough to keep M PD-like.    h = -logabsdet + float(n) * torch.log(torch.tensor(float(s), device=device, dtype=dtype))    # G_A = (sI - A)^{-T}    # Use solve instead of explicit inverse    X = torch.linalg.solve(M, I_n)   # X = M^{-1}    G_A = X.transpose(0, 1)          # M^{-T}    return h, G_Adef _gradW2_from_gradA(G_A: torch.Tensor, d: int) -> torch.Tensor:    """    Given G_A = d objective / dA, extract d objective / dW2,    where W2 appears in A11 and A22^T blocks:      A11 = W2      A22 = W2^T    """    G11 = G_A[:d, :d]    G22 = G_A[d:, d:]    return G11 + G22.transpose(0, 1)# ---------- MAIN: TCC with selectable cycle penalty ----------def trek_cycle_coupling_value_gradW(    W: torch.Tensor,    I: Pairs,    *,    w: float = 1.0,    cycle_penalty: CyclePenalty = "spectral",   # "spectral" | "logdet"    version: TCCVersion = "approx_trek_graph",  # "DAG_learning" | "exact_trek_graph" | "exact_original_graph" | "approx_trek_graph"    # spectral params    method: PerronMethod = "eig_numpy",    idx_mode: str = "single_random",   # "all" | "single_random"    n_iter: int = 50,    # shared / logdet params    s: float = 1.0,    eps: float = 1e-12,) -> tuple[torch.Tensor, torch.Tensor]:    """    Per-pair TCC penalty:      For each (i,j) in I, build S = E_ij, construct A(S), B(S),      compute the chosen cycle penalty (and baseline version), then SUM over pairs.    For scale-compatibility with the previous implementation, we return the AVERAGE    over pairs: (sum penalty)/|I| and (sum grad)/|I|.    Returns:      penalty (scalar tensor)      gradW   (same shape as W)    """    if W.ndim != 2 or W.shape[0] != W.shape[1]:        raise ValueError("W must be square")    if len(I) == 0:        # define as zero penalty/grad if no constraints given        return W.new_zeros(()), torch.zeros_like(W)    d = W.shape[0]    device, dtype = W.device, W.dtype    I_d = torch.eye(d, device=device, dtype=dtype)    # 1) W2 (nonnegative)    W2 = W * W    bot = torch.cat([I_d, W2.transpose(0, 1)], dim=1)  # shared across pairs    # running totals    penalty_total = W.new_zeros(())    grad_total = torch.zeros_like(W)    def _one_pair_indicator(pair) -> torch.Tensor:        """S = E_ij as dense d×d tensor on same device/dtype."""        i, j = pair        S_ij = torch.zeros((d, d), device=device, dtype=dtype)        S_ij[i, j] = 1.0        return S_ij        if idx_mode == "all":        pairs_to_use = list(I)        elif idx_mode == "single_random":        idx = torch.randint(            low=0,            high=len(I),            size=(1,),            generator=_TCC_RNG,            device="cpu",   # generator must match device type        ).item()        pairs_to_use = [list(I)[idx]]        else:        raise ValueError("mode must be one of {'all','single_random'}")    for pair in pairs_to_use:        S_ij = _one_pair_indicator(pair)        zero = torch.zeros_like(S_ij)        # 3) Build A, B for this single pair        A = torch.cat(            [                torch.cat([W2, float(w) * S_ij], dim=1),                bot,            ],            dim=0,        )        B = torch.cat(            [                torch.cat([W2, zero], dim=1),                bot,            ],            dim=0,        )        # 4) Choose cycle penalty for this pair        if cycle_penalty == "spectral":            rho_A, uA, vA, G_A = perron_eig_with_gradA(                A, method=method, n_iter=n_iter, eps=eps            )            G_W2_from_A = _gradW2_from_gradA(G_A, d)            gradW_from_A = 2.0 * W * G_W2_from_A            if version == "DAG_learning":                penalty_ij = rho_A                grad_ij = gradW_from_A            elif version == "exact_trek_graph":                rho_B, uB, vB, G_B = perron_eig_with_gradA(                    B, method=method, n_iter=n_iter, eps=eps                )                G_W2_from_B = _gradW2_from_gradA(G_B, d)                gradW_from_B = 2.0 * W * G_W2_from_B                penalty_ij = rho_A - rho_B                grad_ij = gradW_from_A - gradW_from_B            elif version == "exact_original_graph":                rho_W2, uW2, vW2, G_W2 = perron_eig_with_gradA(                    W2, method=method, n_iter=n_iter, eps=eps                )                # NOTE: here _gradW2_from_gradA is assumed to accept (d×d) too;                # if your helper expects 2d×2d only, replace by identity mapping.                G_W2_from_W2 = _gradW2_from_gradA(G_W2, d)                gradW_from_W2 = 2.0 * W * G_W2_from_W2                penalty_ij = rho_A - rho_W2                grad_ij = gradW_from_A - gradW_from_W2            elif version == "approx_trek_graph":                # Rayleigh lower bound baseline using uA                rho_B_lb = (uA * (B @ uA)).sum() / ((uA * uA).sum() + eps)                den = (uA @ uA) + eps                u1, u2 = uA[:d], uA[d:]                G_W2_lb = (torch.outer(u1, u1) + torch.outer(u2, u2)) / den                gradW_from_B = 2.0 * W * G_W2_lb                penalty_ij = rho_A - rho_B_lb                grad_ij = gradW_from_A - gradW_from_B            else:                raise ValueError("version must be one of {TCCVersion} for spectral")        elif cycle_penalty == "logdet":            h_A, G_A = logdet_acyc_value_gradA(A, s=s, eps=eps)            G_W2_from_A = _gradW2_from_gradA(G_A, d)            gradW_from_A = 2.0 * W * G_W2_from_A            if version == "DAG_learning":                penalty_ij = h_A                grad_ij = gradW_from_A            elif version == "exact_trek_graph":                h_B, G_B = logdet_acyc_value_gradA(B, s=s, eps=eps)                G_W2_from_B = _gradW2_from_gradA(G_B, d)                gradW_from_B = 2.0 * W * G_W2_from_B                penalty_ij = h_A - h_B                grad_ij = gradW_from_A - gradW_from_B            elif version == "exact_original_graph":                raise ValueError(                    "The version 'exact_original_graph' for the 'logdet' acyclicity constraint is not implemented"                )            elif version == "approx_trek_graph":                raise ValueError(                    "The version 'approx_trek_graph' for the 'logdet' acyclicity constraint is not implemented"                )            else:                raise ValueError("version must be one of {TCCVersion} for logdet")        else:            raise ValueError("cycle_penalty must be one of {'spectral','logdet'}")        penalty_total = penalty_total + penalty_ij        grad_total = grad_total + grad_ij    n_vals = float(len(I))    return penalty_total / len(pairs_to_use), grad_total / len(pairs_to_use)def _matrix_power(A: torch.Tensor, p: int) -> torch.Tensor:    """Differentiable matrix power via repeated multiplication."""    if p < 0:        raise ValueError("p must be >= 0")    d = A.shape[0]    I = torch.eye(d, device=A.device, dtype=A.dtype)    if p == 0:        return I    out = A    for _ in range(p - 1):        out = out @ A    return outdef _series_I_minus_log_I_minus_W(        W: torch.Tensor,        K: int,        s:float = 1.) -> torch.Tensor:    """    Compute: I - log(I - W) = I + sum_{k=1..K} W^k / k    This is a truncation of the power series for -log(I - W). It behaves well    when ||W|| is small enough (e.g., spectral radius < 1) and K is sufficiently large.    """    if K < 1:        raise ValueError("K must be >= 1")    d = W.shape[0]    I = torch.eye(d, device=W.device, dtype=W.dtype)    out = I.clone()    Wk = W.clone()    for k in range(1, K + 1):        out = out + Wk / (float(k) * s**k)        Wk = Wk @ W    return outdef pst_mat(    W: torch.Tensor,    seq: PSTPenalty = "exp",    *,    K_log: Optional[int] = None,    eps_inv: float = 1e-8,    s: float = 1.0,) -> torch.Tensor:    """    PST penalty (Torch):      1) Hadamard-square preprocessing: W2 = W ⊙ W  (elementwise square)      2) Compute H(W2) based on seq:         - exp  : H = expm(W2)^T expm(W2)         - log  : H = F^T F, F = I - log(I - W2) via series truncation         - inv  : H = (I - W2)^{-T} (I - W2)^{-1}         - binom: H = ((I + W2)^d)^T (I + W2)^d      3) Return scalar sum_{(i,j) in I} H[i,j]    Returns a torch matrix (keeps gradients w.r.t. W).    """    seq = seq.lower().strip()    if seq not in {"exp", "log", "inv", "binom"}:        raise ValueError("seq must be one of {'exp','log','inv','binom'}")        if not isinstance(W, torch.Tensor):        W = torch.as_tensor(W, dtype=torch.double, device=torch.device("cpu"))    if W.ndim != 2 or W.shape[0] != W.shape[1]:        raise ValueError("W must be a square matrix tensor")    if eps_inv < 0:        raise ValueError("eps_inv must be >= 0")    d = W.shape[0]    device = W.device    dtype = W.dtype    # (1) Hadamard step: elementwise square    W2 = W * W    # (2) build H    if seq == "exp":        F = torch.matrix_exp(W2)        H = F.transpose(0, 1) @ F    elif seq == "inv":        Iden = torch.eye(d, device=device, dtype=dtype)        A = Iden - W2        # small ridge for stability if near singular        if eps_inv > 0:            A = A + eps_inv * Iden        X = torch.linalg.solve(A, Iden)  # (I - W2)^{-1}        H = X.transpose(0, 1) @ X        # (I - W2)^{-T}(I - W2)^{-1}    elif seq == "log":        if K_log is None:            K_log = 2 * int(d)        F = _series_I_minus_log_I_minus_W(W2, K=K_log)        H = F.transpose(0, 1) @ F    elif seq == "binom":        Iden = torch.eye(d, device=device, dtype=dtype)        A = Iden + W2        F = _matrix_power(A, p=int(d))        H = F.transpose(0, 1) @ F    else:        raise ValueError("seq must be one of {'exp','inv','log','binom'")    return Hdef get_no_trek_pairs(    W: torch.Tensor,    seq: PSTPenalty = "exp",    *,    K_log: Optional[int] = None,    eps_inv: float = 1e-8,) -> np.ndarray:    """    Return pairs (i,j) with i < j such that H[i,j] == 0.    (strictly upper triangular only)    """    H = pst_mat(W, seq, K_log=K_log, eps_inv=eps_inv)    with torch.no_grad():        # strictly upper triangle mask        upper = torch.triu(torch.ones_like(H, dtype=torch.bool), diagonal=1)        mask = (H == 0) & upper        rows, cols = torch.nonzero(mask, as_tuple=True)        pairs = torch.stack([rows, cols], dim=1)    return pairs.cpu().numpy().astype(np.int64, copy=False)def pst(    W: torch.Tensor,    I: Pairs,    seq: PSTPenalty = "exp",    *,    K_log: Optional[int] = None,    eps_inv: float = 1e-8,    s: float = 1.,    agg: Agg = "mean",) -> torch.Tensor:    """    PST penalty (Torch):      1) Hadamard-square preprocessing: W2 = W ⊙ W  (elementwise square)      2) Compute H(W2) based on seq:         - exp  : H = expm(W2)^T expm(W2)         - log  : H = F^T F, F = I - log(I - W2) via series truncation         - inv  : H = (I - W2)^{-T} (I - W2)^{-1}         - binom: H = ((I + W2)^d)^T (I + W2)^d    agg:      - "mean" : average over pairs (scale-invariant, recommended)      - "sum"  : sum over pairs      - "max"  : max violation (hard max)      - "lse"  : log-sum-exp smooth max      - "none" : return vector of values    """    agg = agg.lower().strip()    # Handle empty I gracefully    I_np = np.asarray(I, dtype=np.int64)    if I_np.size == 0:        return W.sum() * 0.0    H = pst_mat(W, seq, K_log=K_log, eps_inv=eps_inv, s=s)    device = W.device    rows, cols = _pairs_to_torch(I_np, device)    vals = H[rows, cols]    if vals.numel() == 0:        return H.sum() * 0.0    # -------- aggregation ----------    if agg == "mean":        return vals.mean() if vals.numel() > 0 else H.sum() * 0.0    elif agg == "sum":        return vals.sum()    elif agg == "max":        return vals.max()    elif agg == "lse":  # smooth max        return torch.logsumexp(vals, dim=0)    elif agg == "none":        return vals    else:        raise ValueError("agg must be one of {Agg}")def trek_value(        W: torch.Tensor,        tr: TrekRegularizer) -> torch.Tensor:    """    Compute the trek regularizer value as a torch scalar.    This function is pure w.r.t. W: it assumes W is already the adjacency matrix    representation you want to regularize. It does NOT multiply by tr.weight.    (That is done by the caller/optimizer.)    Supported tr.name:      - "pst"      - "tcc"    """    if tr is None or not tr.enabled():        # return a scalar tensor on same device/dtype (keeps graphs tidy)        return W.sum() * 0.0    name = tr.name.lower().strip()    cfg = tr.cfg if tr.cfg is not None else {}        if len(cfg["I"]) == 0 or cfg["I"] is None:        return 0    if name == "pst":        kwargs = cfg.get("kwargs", {}) or {}        return pst(W,                   cfg["I"],                   seq=cfg.get("seq", "exp"),                   **kwargs)    if name == "tcc":        penalty_t, _ = trek_cycle_coupling_value_gradW(            W,            cfg["I"],            w=cfg.get("w", 1.0),            cycle_penalty=tr.cycle_penalty,            version=cfg["version"],            method=tr.method,            idx_mode=cfg["idx_mode"],            n_iter=cfg.get("n_iter", 10),            s=cfg.get("s", 1.),            eps=cfg.get("eps", 1e-12),        )                return penalty_t    raise ValueError(f"Unknown trek regularizer: {tr.name}. Has to be in {TrekRegularizerNames}")def trek_value_grad(    W: np.ndarray,    tr: Optional[TrekRegularizer],    *,    torch_dtype: torch.dtype = torch.double,    device: Optional[torch.device] = None,) -> Tuple[float, np.ndarray]:    """    Compute trek regularizer (value, grad) w.r.t. W for numpy-based optimizers.    - mode="opt": uses torch autograd to return value + gradient    - mode="log": returns value, gradient=0    - mode="off"/None/disabled: returns 0, gradient=0    Returns:      (value_float, grad_numpy_same_shape_as_W)    """    W_np = np.asarray(W)    if tr is None or not tr.enabled():        return 0.0, np.zeros_like(W_np)        if len(tr.cfg["I"]) == 0 or tr.cfg["I"] is None:        return 0.0, np.zeros_like(W_np)        dev = device if device is not None else torch.device("cpu")    # Create torch tensor    W_t = torch.as_tensor(W_np, dtype=torch_dtype, device=dev)            name = tr.name.lower().strip()    # special: closed-form eigenvector gradient    if name == "tcc":        cfg = tr.cfg        penalty_t, grad_t = trek_cycle_coupling_value_gradW(            W_t,            cfg["I"],            w=cfg.get("w", 1.0),            cycle_penalty=tr.cycle_penalty,            version=cfg.get("version", "approx_trek_graph"),            method=cfg.get("method", "eig_torch"),            idx_mode=cfg.get("idx_mode", "single_random"),            n_iter=cfg.get("n_iter", 10),            eps=cfg.get("eps", 1e-12),        )        val = float(penalty_t.detach().cpu().item())        if tr.mode != "opt":            return val, np.zeros_like(W_np)        return val, grad_t.detach().cpu().numpy().astype(W_np.dtype, copy=False)    # default to autograd    requires_grad = (tr.mode == "opt")    if requires_grad:        # ensure a leaf tensor with grad        W_t = W_t.detach().clone().requires_grad_(True)    # compute value    val_t = trek_value(W_t, tr)    # log-mode: no gradient    if tr.mode != "opt":        return float(val_t.detach().cpu().item()), np.zeros_like(W_np)    # backprop    val_t.backward()    grad_t = W_t.grad    if grad_t is None:        raise RuntimeError("trek grad is None (unexpected).")    return (        float(val_t.detach().cpu().item()),        grad_t.detach().cpu().numpy().astype(W_np.dtype, copy=False),    )def _build_graphs_6(dtype=torch.double, device="cpu"):    """    Build three fixed 6-node graphs:      1) circle: 1→2→3→4→5→6→1      2) path (disconnected): 1→2→3 and 4→5→6  (so (1,4) is NO)      3) zigzag: 1→2←3→4←5→6    Returns dict name -> W (6x6 torch tensor, nonnegative weights).    """    d = 6    def W0():        return torch.zeros((d, d), dtype=dtype, device=device)    # 1) circle    Wc = W0()    for u, v in [(1,2),(2,3),(3,4),(4,5),(5,6),(6,1)]:        Wc[u-1, v-1] = 1.0    # 2) "path" matching your expected (1,4)=NO: two disjoint paths    Wp = W0()    for u, v in [(1,2),(2,3),(4,5),(5,6)]:        Wp[u-1, v-1] = 1.0    # 3) zigzag: 1→2←3→4←5→6    Wz = W0()    for u, v in [(1,2),(3,2),(3,4),(5,4),(5,6)]:        Wz[u-1, v-1] = 1.0    return {"circle": Wc, "path": Wp, "zigzag": Wz}def _sanity_check_structural():    """    Structural sanity check on 6 nodes for 3 graphs and the pairs:      (1,3), (2,3), (1,4)   [1-based indexing]    Expected YES/NO across graphs (circle, path, zigzag), for every method:      (1,3): YES, YES, NO      (2,3): YES, YES, YES      (1,4): YES, NO,  NO    Checks:      1) PST family (exp/log/inv/binom)  [unchanged]      2) TCC family with cycle_penalty:           - spectral  (your Perron / spectral-radius coupling)           - logdet    (DAGMA-style logdet on extended matrix)    """    torch.manual_seed(0)    np.random.seed(0)    graphs = _build_graphs_6(dtype=torch.double, device="cpu")    pairs_1based = [(1, 3), (2, 3), (1, 4)]    expected = {        (1, 3): {"circle": True, "path": True,  "zigzag": False},        (2, 3): {"circle": True, "path": True,  "zigzag": True},        (1, 4): {"circle": True, "path": False, "zigzag": False},    }    tol = 0  # keep consistent with your current choice    def yn(b): return "YES" if b else "NO"    print("=== notreks structural sanity check (PST + TCC) ===")    print("Pairs (1-based):", pairs_1based)    print("Graphs: circle, path, zigzag")    print(f"Rule: YES iff value > {tol:g}")    print()    # ---------------------------    # 1) PST variants (unchanged)    # ---------------------------    seqs = [        ("exp",   dict()),        ("log",   dict(K_log=30)),        ("inv",   dict(eps_inv=1e-8)),        ("binom", dict()),    ]    for seq, kwargs in seqs:        print(f"--- pst seq = {seq} ---")        ok_all = True        for (i, j) in pairs_1based:            got = {}            for g in ["circle", "path", "zigzag"]:                W = graphs[g]                val = pst(W, [(i-1, j-1)], seq=seq, **kwargs)  # 0-based                got[g] = (val.item() > tol)            exp = expected[(i, j)]            match = all(got[g] == exp[g] for g in exp.keys())            ok_all = ok_all and match            print(                f"pair ({i},{j}): "                f"circle={yn(got['circle'])}, path={yn(got['path'])}, zigzag={yn(got['zigzag'])} "                f"| expected: circle={yn(exp['circle'])}, path={yn(exp['path'])}, zigzag={yn(exp['zigzag'])} "                f"| {'OK' if match else 'MISMATCH'}"            )        print("Result:", "ALL OK" if ok_all else "SOME MISMATCHES")        print()    # ---------------------------    # 2) TCC family: spectral + logdet    # ---------------------------    def _run_tcc_block(title: str, **tcc_kwargs):        print(f"--- {title} ---")        ok_all = True        for (i, j) in pairs_1based:            got = {}            for g in ["circle", "path", "zigzag"]:                W = graphs[g]                val, _ = trek_cycle_coupling_value_gradW(                    W,                    [(i-1, j-1)],   # 0-based                    **tcc_kwargs,                )                got[g] = (val.item() > tol)            exp = expected[(i, j)]            match = all(got[g] == exp[g] for g in exp.keys())            ok_all = ok_all and match            print(                f"pair ({i},{j}): "                f"circle={yn(got['circle'])}, path={yn(got['path'])}, zigzag={yn(got['zigzag'])} "                f"| expected: circle={yn(exp['circle'])}, path={yn(exp['path'])}, zigzag={yn(exp['zigzag'])} "                f"| {'OK' if match else 'MISMATCH'}"            )        print("Result:", "ALL OK" if ok_all else "SOME MISMATCHES")        print()    # 2a) TCC spectral (your previous tcc, renamed)    # choose reasonably stable defaults    _run_tcc_block(        "tcc cycle_penalty=spectral",        cycle_penalty="spectral",        version="approx_trek_graph",          # your default        w=1.0,        method="eig_numpy",   # or "power"        n_iter=10,        eps=1e-12,        s=1.0,                # unused in spectral, harmless    )    # 2b) TCC logdet    # NOTE: logdet needs s large enough so (sI - A) is well-conditioned/invertible.    # For these tiny graphs s=2.0 is typically safe, but if you see sign flips / NaNs,    # bump s (e.g. 5.0 or 10.0).    _run_tcc_block(        "tcc cycle_penalty=logdet",        cycle_penalty="logdet",        version="exact_trek_graph",   # exact baseline (logdet has no Rayleigh-lb analogue)        w=1.0,        s=2.0,        eps=1e-12,        # spectral-only args below are ignored by the logdet branch in our implementation        method="eig_numpy",        n_iter=10,    )    print("=== done ===")if __name__ == "__main__":    _sanity_check_structural()