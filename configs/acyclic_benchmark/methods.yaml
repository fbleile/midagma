methods:    # - id: dagma_linear_base    #   algo:    #     name: dagma_linear    #     lambda1: .01    #     max_iter: 60000    #     mu_factor: 0.1    #     s: 1.    #   trek_reg:    #     name: none    # - id: dagma_linear_exp    #   algo:    #     name: dagma_linear    #     lambda1: .01    #     max_iter: 60000    #     mu_factor: 0.1    #     s: 1.    #   trek_reg:    #     name: pst    #     weight: 1.    #     mode: opt    #     seq: exp    # - id: dagma_linear_inv    #   algo:    #     name: dagma_linear    #     lambda1: .01    #     max_iter: 60000    #     mu_factor: 0.1    #     s: 1.    #   trek_reg:    #     name: pst    #     weight: .01    #     mode: opt    #     seq: inv    # - id: dagma_linear_log    #   algo:    #     name: dagma_linear    #     lambda1: .01    #     max_iter: 60000    #     mu_factor: 0.1    #     s: 1.    #   trek_reg:    #     name: pst    #     weight: .01    #     mode: opt    #     seq: log    - id: dagma_linear_tcc      algo:        name: dagma_linear        lambda1: .01        max_iter: 60000        mu_factor: 0.1        s: 1.      trek_reg:        name: tcc        weight: # [.000001, .00001, .0001, .001]        mode: opt        cycle_penalty: spectral        version: # [DAG_learning, approx_trek_graph]        w: 100.        idx_mode: # [single_random, node_random]    # - id: notears_linear_base    #   algo:    #     name: notears_linear    #     lambda1: 0.0    #     loss_type: l2    #     max_iter: 100    #     h_tol: 1.e-8    #     rho_max: 1.e+16    #     w_threshold: .5    #   trek_reg:    #     name: none    # - id: notears_linear_exp    #   algo:    #     name: notears_linear    #     lambda1: 0.0    #     loss_type: l2    #     max_iter: 100    #     h_tol: 1.e-8    #     rho_max: 1.e+16    #     w_threshold: .5    #   trek_reg:    #     name: pst    #     weight: 1.    #     mode: opt    #     seq: exp    # - id: notears_linear_inv    #   algo:    #     name: notears_linear    #     lambda1: 0.0    #     loss_type: l2    #     max_iter: 100    #     h_tol: 1.e-8    #     rho_max: 1.e+16    #     w_threshold: .5    #   trek_reg:    #     name: pst    #     weight: 1.    #     mode: opt    #     seq: inv    # - id: notears_linear_log    #   algo:    #     name: notears_linear    #     lambda1: 0.0    #     loss_type: l2    #     max_iter: 100    #     h_tol: 1.e-8    #     rho_max: 1.e+16    #     w_threshold: .5    #   trek_reg:    #     name: pst    #     weight: 1.    #     mode: opt    #     seq: log    - id: notears_linear_tcc      algo:        name: notears_linear        lambda1: 0.0        loss_type: l2        max_iter: 100        h_tol: 1.e-8        rho_max: 1.e+16        w_threshold: .5      trek_reg:        name: tcc        weight: # [.000001, .00001, .0001, .001]        mode: opt        cycle_penalty: spectral        version: # [DAG_learning, approx_trek_graph]        w: 100.        idx_mode: # [single_random, node_random]                # rm -rf cmd.txt# python src/experiment/experiment_manager.py acyclic_benchmark --methods --n_datasets=300 --compute=cluster --submit# sbatch jobfarm.sh# rm -rf cmd.txt# python src/experiment/experiment_manager.py acyclic_benchmark --summary --n_datasets=300 --compute=cluster --submit# sbatch jobfarm.sh